{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/explainpark101/2025-2-DL/blob/main/reinforced_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec4f4fe",
      "metadata": {
        "id": "7ec4f4fe"
      },
      "source": [
        "# Create Environment of Blackjack"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gymnasium>=1.2.2\" --user -q"
      ],
      "metadata": {
        "id": "L-p5oK8FkqQD"
      },
      "id": "L-p5oK8FkqQD",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "10cd89f8",
      "metadata": {
        "id": "10cd89f8"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from gymnasium.envs.toy_text.blackjack import BlackjackEnv as InfiniteDeckBlackjackEnv\n",
        "\n",
        "@dataclass\n",
        "class BlackJackRewards:\n",
        "    BUST: float = -5.0\n",
        "    LOSS: float = -3.0\n",
        "    WIN: float = 3.0\n",
        "    NATURAL_BLACKJACK: float = 4.5\n",
        "    DRAW: float = 0.0\n",
        "\n",
        "class BlackjackEnv(InfiniteDeckBlackjackEnv):\n",
        "    def __init__(self, render_mode=None, num_decks=6, rewards: BlackJackRewards = BlackJackRewards(), natural=True, sab=False):\n",
        "        # 부모 클래스 초기화 (기본 설정 유지)\n",
        "        super().__init__(render_mode=render_mode, sab=sab, natural=natural)\n",
        "\n",
        "        self.num_decks = num_decks\n",
        "        self.deck = []\n",
        "        self.reshuffle_deck()\n",
        "\n",
        "        self.rewards = rewards\n",
        "\n",
        "    def reshuffle_deck(self):\n",
        "        # 1~13 (A~K) 카드를 4무늬 * N덱 만큼 생성\n",
        "        # Gymnasium Blackjack은 J,Q,K를 모두 10으로 처리하므로 그에 맞춰 구성\n",
        "        # 1(Ace), 2~9, 10(10,J,Q,K) 비율 반영\n",
        "        deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10] * 4 * self.num_decks\n",
        "        self.np_random.shuffle(deck)\n",
        "        self.deck = deck\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, original_reward, terminated, truncated, info = super().step(action)\n",
        "\n",
        "        final_reward = original_reward\n",
        "\n",
        "        # 게임이 끝난 경우(terminated)에만 보상 수정 로직 적용\n",
        "        if terminated or truncated:\n",
        "            if action == 1: # 상황 1: Hit(1)을 했는데 게임이 끝남 -> 무조건 Bust\n",
        "                final_reward = self.rewards.BUST\n",
        "\n",
        "            elif action == 0: # 상황 2, 3, 4: Stay(0)를 해서 게임이 끝남 -> 결과 비교\n",
        "                if original_reward == 1.5: # natural blackjack\n",
        "                    final_reward = self.rewards.NATURAL_BLACKJACK\n",
        "                elif original_reward > 0:\n",
        "                    final_reward = self.rewards.WIN\n",
        "                elif original_reward < 0:\n",
        "                    final_reward = self.rewards.LOSS\n",
        "                else:\n",
        "                    final_reward = self.rewards.DRAW\n",
        "\n",
        "        return next_state, final_reward, terminated, truncated, info\n",
        "\n",
        "    def draw_card(self):\n",
        "        if len(self.deck) < 1: # 댁 다 쓰면 다음 라운드\n",
        "            self.reshuffle_deck()\n",
        "        return self.deck.pop()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # 덱 초기화가 필요하면 여기서 처리\n",
        "        return super().reset(seed=seed, options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ac2fa21",
      "metadata": {
        "id": "6ac2fa21"
      },
      "source": [
        "# Create Neural Network and Replay Buffer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f7120f59",
      "metadata": {
        "id": "f7120f59"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9a63f3c4",
      "metadata": {
        "id": "9a63f3c4"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Input: Player Sum, Dealer Card, Usable Ace (3 dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(3, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2) # Output: Stay(0), Hit(1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "52daa92b",
      "metadata": {
        "id": "52daa92b"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b882269d",
      "metadata": {
        "id": "b882269d"
      },
      "outputs": [],
      "source": [
        "class DDQNAgent:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.policy_net = QNetwork().to(device)\n",
        "        self.target_net = QNetwork().to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
        "        self.memory = ReplayBuffer(10000)\n",
        "\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "    def preprocess(self, state):\n",
        "        # Tuple State (Sum, Dealer) -> Tensor\n",
        "        # Ace는 boolean이므로 float으로 변환\n",
        "        state_arr = np.array([state[0], state[1], state[2]], dtype=np.float32)\n",
        "        return torch.FloatTensor(state_arr).unsqueeze(0).to(self.device)\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice([0, 1])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_t = self.preprocess(state)\n",
        "                q_values = self.policy_net(state_t)\n",
        "                return torch.argmax(q_values).item()\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return 0.0\n",
        "        states: tuple[int, int]\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert to Tensor\n",
        "        states = torch.FloatTensor(np.array([\n",
        "            [s[0], s[1], s[2]] for s in states\n",
        "        ])).to(self.device)\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.array([\n",
        "            [ns[0], ns[1], ns[2]] for ns in next_states\n",
        "        ])).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # DDQN Logic\n",
        "        # 1. Main Net에서 다음 상태의 최적 행동 선택 (argmax)\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.policy_net(next_states).argmax(1).unsqueeze(1)\n",
        "            # 2. Target Net에서 그 행동의 Q값 계산\n",
        "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
        "            # 3. Target 값 계산\n",
        "            target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
        "\n",
        "        # Current Q\n",
        "        current_q_values = self.policy_net(states).gather(1, actions)\n",
        "\n",
        "        # Compute Loss & Update\n",
        "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def update_target_net(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e6e1c10",
      "metadata": {
        "id": "4e6e1c10"
      },
      "source": [
        "# Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d6af917e",
      "metadata": {
        "id": "d6af917e",
        "outputId": "35e3c4b5-ba17-497a-b576-51cf70515a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/50000 [00:00<?, ?ep/s]"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "rewards = BlackJackRewards(\n",
        "    BUST = -5.0,\n",
        "    LOSS = -3.0,\n",
        "    WIN = 3.0,\n",
        "    NATURAL_BLACKJACK = 3.5,\n",
        "    DRAW = 0.0\n",
        ")\n",
        "env = BlackjackEnv(num_decks=6, rewards=rewards, natural=True, sab=True)\n",
        "agent = DDQNAgent(device)\n",
        "\n",
        "num_episodes = 50_000\n",
        "target_update_freq = 100\n",
        "\n",
        "\n",
        "recent_rewards = deque(maxlen=100)\n",
        "recent_losses = deque(maxlen=100)\n",
        "\n",
        "# progress bar | not for learning, but for visualization of learning\n",
        "pbar = tqdm(range(num_episodes), desc=\"Training\", unit=\"ep\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ede37a5",
      "metadata": {
        "id": "7ede37a5",
        "outputId": "e236fc35-d689-48c5-8357-b40847809f5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   9%|▉         | 4471/50000 [00:43<10:04, 75.27ep/s, Avg Reward=-0.17, Loss=5.4610, Eps=0.01]"
          ]
        }
      ],
      "source": [
        "def train():\n",
        "    for episode in pbar:\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        loss = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            loss = agent.train()\n",
        "\n",
        "        # Update params\n",
        "        agent.update_epsilon()\n",
        "        if episode % target_update_freq == 0:\n",
        "            agent.update_target_net()\n",
        "\n",
        "        # Logging\n",
        "        recent_rewards.append(total_reward)\n",
        "        if loss != 0: recent_losses.append(loss)\n",
        "\n",
        "        avg_reward = np.mean(recent_rewards) if recent_rewards else 0.0\n",
        "        avg_loss = np.mean(recent_losses) if recent_losses else 0.0\n",
        "\n",
        "        # tqdm bar update\n",
        "        pbar.set_postfix({\n",
        "            'Avg Reward': f'{avg_reward:.2f}',\n",
        "            'Loss': f'{avg_loss:.4f}',\n",
        "            'Eps': f'{agent.epsilon:.2f}'\n",
        "        })\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e395c4f",
      "metadata": {
        "id": "6e395c4f"
      },
      "source": [
        "## Model Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e3610d",
      "metadata": {
        "id": "b6e3610d"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.policy_net.state_dict(), \"ddqn_blackjack.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7665475c",
      "metadata": {
        "id": "7665475c"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccac1006",
      "metadata": {
        "id": "ccac1006"
      },
      "outputs": [],
      "source": [
        "\"import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Load Model & Environment ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = QNetwork().to(device)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"ddqn_blackjack.pth\"))\n",
        "except FileNotFoundError:\n",
        "    print(\"모델 파일이 없습니다. 학습을 먼저 진행해주세요.\")\n",
        "    exit()\n",
        "\n",
        "model.eval() # 평가 모드 (Dropout/BatchNorm 등 비활성화)\n",
        "env = BlackjackEnv(num_decks=6)\n",
        "\n",
        "# --- 2. Helper Function for Inference ---\n",
        "def get_ai_action(state):\n",
        "    # State: (PlayerSum, DealerShow, UsableAce)\n",
        "    state_arr = np.array([state[0], state[1], state[2]], dtype=np.float32)\n",
        "    state_t = torch.FloatTensor(state_arr).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        q_values = model(state_t)\n",
        "        return torch.argmax(q_values).item() # 0: Stay, 1: Hit\n",
        "\n",
        "# --- 3. Testing Loop ---\n",
        "def test_play(num_games=10, print_result=False):\n",
        "    wins = 0\n",
        "    draws = 0\n",
        "    losses = 0\n",
        "    if not print_result:\n",
        "        def print(*args, **kwargs):\n",
        "            pass\n",
        "    print(f\"\\n--- AI 상세 테스트 시작 ({num_games} 게임) ---\")\n",
        "\n",
        "    for i in range(num_games):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        print(f\"\\n[Game {i+1}]\")\n",
        "\n",
        "        # Player Turn\n",
        "        while not done:\n",
        "            player_sum, dealer_show, usable_ace = state\n",
        "\n",
        "            # --- 수정된 부분: 현재 패 정보를 env 객체에서 직접 가져옴 ---\n",
        "            current_player_hand = env.player  # 예: [10, 5]\n",
        "\n",
        "            action = get_ai_action(state)\n",
        "            action_str = \"Hit\" if action == 1 else \"Stay\"\n",
        "\n",
        "            print(f\"  Turn: Player {current_player_hand} ({player_sum}) vs Dealer Show [{dealer_show}] -> Action: {action_str}\")\n",
        "\n",
        "            if action == 0: # Stay\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "            else: # Hit\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # --- Game Over: 최종 결과 및 딜러의 숨겨진 패 공개 ---\n",
        "        card_map = {\n",
        "            1: 'A',\n",
        "            11: 'J',\n",
        "            12: 'Q',\n",
        "            13: \"K\",\n",
        "            **{i:i for i in range(2, 11)}\n",
        "        }\n",
        "        final_player_hand = env.player\n",
        "        final_dealer_hand = env.dealer # 게임이 끝나면 딜러의 모든 카드가 공개됨\n",
        "        final_player_hand_show = list(card_map.get(card) for card in env.player)\n",
        "        final_dealer_hand_show = list(card_map.get(card) for card in env.dealer) # 게임이 끝나면 딜러의 모든 카드가 공개됨\n",
        "\n",
        "        # 승패 판정 로직\n",
        "        if reward == rewards.NATURAL_BLACKJACK or reward == rewards.WIN:\n",
        "            result = \"WIN\"\n",
        "            wins += 1\n",
        "        elif reward == rewards.LOSS or reward == rewards.BUST:\n",
        "            result = \"LOSS\"\n",
        "            losses += 1\n",
        "        elif reward == rewards.DRAW:\n",
        "            result = \"DRAW\"\n",
        "            draws += 1\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid reward: {reward}\")\n",
        "\n",
        "        print(f\"  Result: {result} (Reward: {reward})\")\n",
        "        print(f\"  >> Final Hands: Player {final_player_hand_show} ({sum(final_player_hand)}) vs Dealer {final_dealer_hand_show} ({sum(final_dealer_hand)})\")\n",
        "\n",
        "    # --- Final Stats ---\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(f\"총 전적: {wins}승 {draws}무 {losses}패\")\n",
        "    print(f\"승률: {wins/num_games*100:.1f}% (무승부 포함 {(wins+draws)/num_games*100:.1f}%)\")\n",
        "    print(\"=\"*30)\n",
        "    return wins/num_games\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    total_days = 10\n",
        "    win_rates = []\n",
        "    for _ in range(total_days):\n",
        "        win_rate = test_play(30, print_result=False)\n",
        "        win_rates.append(win_rate)\n",
        "    print(f\"{total_days}테이블 동안의 승률: {np.mean(win_rates)*100:.1f}%\")\n",
        "    print(f\"최고 승률: {max(win_rates)*100:.1f}%\")\n",
        "    print(f\"최저 승률: {min(win_rates)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb6411d",
      "metadata": {
        "id": "ceb6411d"
      },
      "source": [
        "# Startegy Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e01a07",
      "metadata": {
        "id": "62e01a07"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Load Model ---\n",
        "# (QNetwork 클래스가 정의되어 있어야 함)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = QNetwork().to(device)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"ddqn_blackjack.pth\"))\n",
        "    model.eval()\n",
        "except FileNotFoundError:\n",
        "    print(\"모델 파일이 없습니다.\")\n",
        "    exit()\n",
        "\n",
        "def get_best_action(player_sum, dealer_card, usable_ace):\n",
        "    # Model Input: [PlayerSum, DealerCard, UsableAce]\n",
        "    state = torch.FloatTensor([player_sum, dealer_card, int(usable_ace)]).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(state)\n",
        "        return torch.argmax(q_values).item() # 0: Stay, 1: Hit\n",
        "\n",
        "# --- 2. Generate Strategy Grids ---\n",
        "# Dealer Card: 1(Ace) ~ 10\n",
        "dealer_cards = range(1, 11)\n",
        "dealer_labels = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
        "\n",
        "# Hard Hand: Player Sum 4 ~ 21 (Ace 없음 or Ace=1)\n",
        "hard_sums = range(21, 3, -1) # 21부터 4까지 역순 (그래프 위쪽이 높은 숫자)\n",
        "hard_grid = []\n",
        "\n",
        "for p_sum in hard_sums:\n",
        "    row = []\n",
        "    for d_card in dealer_cards:\n",
        "        action = get_best_action(p_sum, d_card, usable_ace=False)\n",
        "        row.append(action)\n",
        "    hard_grid.append(row)\n",
        "\n",
        "# Soft Hand: Player Sum 12 ~ 21 (Usable Ace 존재)\n",
        "soft_sums = range(21, 11, -1)\n",
        "soft_grid = []\n",
        "\n",
        "for p_sum in soft_sums:\n",
        "    row = []\n",
        "    for d_card in dealer_cards:\n",
        "        action = get_best_action(p_sum, d_card, usable_ace=True)\n",
        "        row.append(action)\n",
        "    soft_grid.append(row)\n",
        "\n",
        "# --- 3. Plot Heatmaps ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Helper to plot\n",
        "def plot_strategy(ax, grid, y_labels, title):\n",
        "    sns.heatmap(grid, cmap=\"coolwarm_r\", annot=True, fmt=\"d\", cbar=False,\n",
        "                xticklabels=dealer_labels, yticklabels=y_labels,\n",
        "                linewidths=.5, ax=ax)\n",
        "    ax.set_title(title, fontsize=15)\n",
        "    ax.set_xlabel(\"Dealer Show Card\")\n",
        "    ax.set_ylabel(\"Player Sum\")\n",
        "    # 0(Red/Stay), 1(Blue/Hit)\n",
        "\n",
        "plot_strategy(axes[0], hard_grid, hard_sums, \"Hard Hand Strategy (No Usable Ace)\")\n",
        "plot_strategy(axes[1], soft_grid, soft_sums, \"Soft Hand Strategy (Usable Ace)\")\n",
        "\n",
        "# Legend 추가\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor='#3b4cc0', label='Hit (1)'),\n",
        "                   Patch(facecolor='#b40426', label='Stay (0)')]\n",
        "fig.legend(handles=legend_elements, loc='upper center', ncol=2, fontsize=12)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2V7TyzPVliN4"
      },
      "id": "2V7TyzPVliN4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}